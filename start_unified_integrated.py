#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å•Ÿå‹•æ•´åˆå¾Œçš„çµ±ä¸€ä¼åŠƒéœ€æ±‚åŠ©æ‰‹æœå‹™
æ•´åˆäº†æ‰€æœ‰åŠŸèƒ½ï¼šä¼åŠƒå°ˆæ¡ˆç®¡ç†ã€å—çœ¾æ•™ç·´ã€å°è©±å¼éœ€æ±‚æ”¶é›†
è‡ªå‹•å•Ÿå‹• Ollama æœå‹™ä¸¦è¼‰å…¥ gemma3:27b æ¨¡å‹
"""

import subprocess
import time
import requests
import sys
import os
from pathlib import Path


def check_ollama_service():
    """æª¢æŸ¥ Ollama æœå‹™æ˜¯å¦é‹è¡Œ"""
    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        if response.status_code == 200:
            models = response.json().get("models", [])
            print(f"âœ… Ollama æœå‹™æ­£å¸¸é‹è¡Œ")
            print(f"ğŸ“‹ å¯ç”¨æ¨¡å‹: {[model['name'] for model in models]}")
            return True
        else:
            print(f"âŒ Ollama æœå‹™å›æ‡‰ç•°å¸¸: {response.status_code}")
            return False
    except Exception as e:
        print(f"âŒ ç„¡æ³•é€£æ¥åˆ° Ollama æœå‹™: {e}")
        return False


def start_ollama_service():
    """å•Ÿå‹• Ollama æœå‹™"""
    print("ğŸš€ å•Ÿå‹• Ollama æœå‹™...")

    try:
        # æª¢æŸ¥ Ollama æ˜¯å¦å·²å®‰è£
        result = subprocess.run(["ollama", "--version"], capture_output=True, text=True)
        if result.returncode != 0:
            print("âŒ Ollama æœªå®‰è£ï¼Œè«‹å…ˆå®‰è£ Ollama")
            print("ğŸ’¡ å®‰è£æŒ‡å—: https://ollama.ai/download")
            return False

        print("âœ… Ollama å·²å®‰è£")

        # å•Ÿå‹• Ollama æœå‹™
        print("ğŸ”„ å•Ÿå‹• Ollama æœå‹™...")
        subprocess.Popen(
            ["ollama", "serve"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL
        )

        # ç­‰å¾…æœå‹™å•Ÿå‹•
        print("â³ ç­‰å¾… Ollama æœå‹™å•Ÿå‹•...")
        for i in range(30):  # æœ€å¤šç­‰å¾…30ç§’
            if check_ollama_service():
                print("âœ… Ollama æœå‹™å•Ÿå‹•æˆåŠŸ")
                return True
            time.sleep(1)
            if i % 5 == 0:
                print(f"â³ ç­‰å¾…ä¸­... ({i+1}/30)")

        print("âŒ Ollama æœå‹™å•Ÿå‹•è¶…æ™‚")
        return False

    except Exception as e:
        print(f"âŒ å•Ÿå‹• Ollama æœå‹™å¤±æ•—: {e}")
        return False


def ensure_gemma_model():
    """ç¢ºä¿ gemma3:27b æ¨¡å‹å¯ç”¨"""
    try:
        print("ğŸ” æª¢æŸ¥ gemma3:27b æ¨¡å‹...")
        response = requests.get("http://localhost:11434/api/tags", timeout=10)
        if response.status_code == 200:
            models = response.json().get("models", [])
            model_names = [model["name"] for model in models]

            if "gemma3:27b" in model_names:
                print("âœ… gemma3:27b æ¨¡å‹å·²å¯ç”¨")
                return True
            else:
                print("ğŸ“¥ ä¸‹è¼‰ gemma3:27b æ¨¡å‹...")
                print("ğŸ’¡ é€™å¯èƒ½éœ€è¦å¹¾åˆ†é˜æ™‚é–“ï¼Œè«‹è€å¿ƒç­‰å¾…...")

                # ä¸‹è¼‰æ¨¡å‹
                subprocess.run(
                    ["ollama", "pull", "gemma3:27b"],
                    check=True,
                    stdout=subprocess.DEVNULL,
                    stderr=subprocess.DEVNULL,
                )

                print("âœ… gemma3:27b æ¨¡å‹ä¸‹è¼‰å®Œæˆ")
                return True
        else:
            print(f"âŒ ç„¡æ³•ç²å–æ¨¡å‹åˆ—è¡¨: {response.status_code}")
            return False

    except Exception as e:
        print(f"âŒ æª¢æŸ¥/ä¸‹è¼‰æ¨¡å‹å¤±æ•—: {e}")
        return False


def check_dependencies():
    """æª¢æŸ¥ä¾è³´æ˜¯å¦å·²å®‰è£"""
    try:
        import fastapi
        import uvicorn
        import pydantic
        import requests

        print("âœ… æ‰€æœ‰ä¾è³´å·²å®‰è£")
        return True
    except ImportError as e:
        print(f"âŒ ç¼ºå°‘ä¾è³´: {e}")
        print("è«‹åŸ·è¡Œ: pip install -r requirements.txt")
        return False


def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸš€ å•Ÿå‹•æ•´åˆå¾Œçš„çµ±ä¸€ä¼åŠƒéœ€æ±‚åŠ©æ‰‹æœå‹™")
    print("ğŸ“ ç‰ˆæœ¬: 2.0.0 (æ•´åˆç‰ˆ)")
    print("=" * 60)

    # æª¢æŸ¥ä¾è³´
    if not check_dependencies():
        return 1

    # æª¢æŸ¥ Ollama æœå‹™
    if not check_ollama_service():
        print("\nğŸ”„ å˜—è©¦å•Ÿå‹• Ollama æœå‹™...")
        if not start_ollama_service():
            print("âŒ ç„¡æ³•å•Ÿå‹• Ollama æœå‹™")
            print("ğŸ’¡ è«‹æ‰‹å‹•å•Ÿå‹•: ollama serve")
            return 1

    # ç¢ºä¿ gemma3:27b æ¨¡å‹å¯ç”¨
    if not ensure_gemma_model():
        print("âŒ ç„¡æ³•ç¢ºä¿ gemma3:27b æ¨¡å‹å¯ç”¨")
        return 1

    print("\nğŸŒ å•Ÿå‹• FastAPI æœå‹™å™¨...")
    print(f"ğŸ“¡ API ç«¯é»: http://localhost:8000")
    print(f"ğŸ“– API æ–‡æª”: http://localhost:8000/docs")
    print(f"ğŸ” å¥åº·æª¢æŸ¥: http://localhost:8000/health")
    print("\nğŸ’¡ æ•´åˆåŠŸèƒ½:")
    print("  - ä¼åŠƒå°ˆæ¡ˆç®¡ç† (åŸæœ‰åŠŸèƒ½)")
    print("  - å—çœ¾æ•™ç·´ç³»çµ± (å¢å¼·ç‰ˆ)")
    print("  - å°è©±å¼éœ€æ±‚æ”¶é›† (/chat/message)")
    print("  - AI è‡ªå‹•è£œå…¨ (/chat/autofill)")
    print("  - æœƒè©±ç®¡ç† (/chat/sessions)")
    print("  - å—çœ¾æ´å¯Ÿåˆ†æ (/audience-coach/insights)")
    print("  - å—çœ¾ç­–ç•¥å»ºè­° (/audience-coach/strategy)")
    print("\næŒ‰ Ctrl+C åœæ­¢æœå‹™")
    print("-" * 60)

    try:
        # å•Ÿå‹• FastAPI æœå‹™å™¨
        import uvicorn

        uvicorn.run("app:app", host="0.0.0.0", port=8000, reload=True, log_level="info")
    except KeyboardInterrupt:
        print("\nğŸ‘‹ æœå‹™å™¨å·²åœæ­¢")
        return 0
    except Exception as e:
        print(f"âŒ å•Ÿå‹•æœå‹™å™¨å¤±æ•—: {e}")
        return 1


if __name__ == "__main__":
    sys.exit(main())
